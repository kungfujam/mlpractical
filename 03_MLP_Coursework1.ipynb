{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kungfujam/anaconda/lib/python2.7/site-packages/IPython/parallel.py:13: ShimWarning: The `IPython.parallel` package has been deprecated. You should import from ipyparallel instead.\n",
      "  \"You should import from ipyparallel instead.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "# Admin functions\n",
    "# https://ipython.org/ipython-doc/dev/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "# When modules are changed externally, uses latest version\n",
    "%autoreload 2\n",
    "# Lanches a console for interaction with this kernel (for testing)\n",
    "%qtconsole\n",
    "# If you get an error here, try running conda install ipyparallel and running again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework #1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This coursework is concerned with building multi-layer networks to address the MNIST digit classification problem. It builds on the previous labs, in particular [02_MNIST_SLN.ipynb](02_MNIST_SLN.ipynb) in which single layer networks were trained for MNIST digit classification.   The course will involve extending that code to use Sigmoid and Softmax layers, combining these into multi-layer networks, and carrying out a number of MNIST digit classification experiments, to investigate the effect of learning rate, the number of hidden units, and the number of hidden layers.\n",
    "\n",
    "The coursework is divided into 4 tasks:\n",
    "* **Task 1**:   *Implementing a sigmoid layer* - 15 marks.  \n",
    "This task involves extending the `Linear` class in file `mlp/layers.py` to `Sigmoid`, with code for forward prop, backprop computation of the gradient, and weight update.\n",
    "* **Task 2**:  *Implementing a softmax layer* - 15 marks.  \n",
    "This task involves extending the `Linear` class in file `mlp/layers.py` to `Softmax`, with code for forward prop, backprop computation of the gradient, and weight update.\n",
    "* **Task 3**:  *Constructing a multi-layer network* - 40 marks.  \n",
    "This task involves putting together a Sigmoid and a Softmax layer to create a multi-layer network, with one hidden layer (100 units) and one output layer, that is trained to classify MNIST digits.  This task will include reporting classification results, exploring the effect of learning rates, and plotting Hinton Diagrams for the hidden units and output units.\n",
    "* **Task 4**:  *Experiments with different architectures*  - 30 marks.  \n",
    "This task involves further MNIST classification experiments, primarily looking at the effect of using different numbers of hidden layers.\n",
    "The coursework will be marked out of 100, and will contribute 30% of the total mark in the MLP course.\n",
    "\n",
    "## Previous Tutorials\n",
    "\n",
    "Before starting this coursework make sure that you have completed the first three labs:\n",
    "\n",
    "* [00_Introduction.ipynb](00_Introduction.ipynb) - setting up your environment; *Solutions*: [00_Introduction_solution.ipynb](00_Introduction_solution.ipynb)\n",
    "* [01_Linear_Models.ipynb](01_Linear_Models.ipynb) - training single layer networks; *Solutions*: [01_Linear_Models_solution.ipynb](01_Linear_Models_solution.ipynb)\n",
    "* [02_MNIST_SLN.ipynb](02_MNIST_SLN.ipynb) - training a single layer network for MNIST digit classification\n",
    "\n",
    "To ensure that your virtual environment is correct, please see [this note](https://github.com/CSTR-Edinburgh/mlpractical/blob/master/kernel_issue_fix.md) on the GitHub.\n",
    "## Submission\n",
    "**Submission Deadline:  Thursday 29 October, 16:00** \n",
    "\n",
    "Submit the coursework as an ipython notebook file, using the `submit` command in the terminal on a DICE machine. If your file is `03_MLP_Coursework1.ipynb` then you would enter:\n",
    "\n",
    "`submit mlp 1 03_MLP_Coursework1.ipynb` \n",
    "\n",
    "where `mlp 1` indicates this is the first coursework of MLP.\n",
    "\n",
    "After submitting, you should receive an email of acknowledgment from the system confirming that your submission has been received successfully. Keep the email as evidence of your coursework submission.\n",
    "\n",
    "**Please make sure you submit a single `ipynb` file (and nothing else)!**\n",
    "\n",
    "**Submission Deadline:  Thursday 29 October, 16:00** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Please enter your exam number and the date in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP Coursework 1\n",
    "#Exam number: B058714\n",
    "#Date: 20/10/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the next code cell, which imports `numpy` and seeds the random number generator.  Please **do not** modify the random number generator seed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "#Seed a random number generator running the below cell, but do **not** modify the seed.\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "rng_state = rng.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Sigmoid Layer (15%)\n",
    "\n",
    "In this task you need to create a class `Sigmoid` which encapsulates a layer of sigmoid units.  You should do this by extending the `mlp.layers.Linear` class (in file `mlp/layers.py`), which implements a layer of linear units (i.e. weighted sum plus bias).  The `Sigmoid` class extends this by applying the sigmoid transfer function to the weighted sum in the forward propagation, and applying the derivative of the sigmoid in the gradient descent back propagation and computing the gradients with respect to layer's parameters. Do **not** copy the implementation provided in `Linear` class but rather, **reuse** it through inheritance.\n",
    "\n",
    "When you have implemented `Sigmoid` (in the `mlp.layers` module), then please test it by running the below code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mlp.layers import Sigmoid\n",
    "\n",
    "a = numpy.asarray([-20.1, 52.4, 0, 0.05, 0.05, 49])\n",
    "b = numpy.asarray([-20.1, 52.4, 0, 0.05, 0.05, 49, 20, 20])\n",
    "\n",
    "rng.set_state(rng_state)\n",
    "sigm = Sigmoid(idim=a.shape[0], odim=b.shape[0], rng=rng)\n",
    "\n",
    "fp = sigm.fprop(a)\n",
    "deltas, ograds  = sigm.bprop(h=fp, igrads=b)\n",
    "\n",
    "print fp.sum()\n",
    "print deltas.sum()\n",
    "print ograds.sum()\n",
    "%precision 3\n",
    "print fp\n",
    "print deltas\n",
    "print ograds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "To include the `Sigmoid` code in the notebook please run the below code cell.  (The `%load` notebook command is used to load the source of the `Sigmoid` class from `mlp/layers.py`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load -s Sigmoid mlp/layers.py\n",
    "class Sigmoid(Linear):\n",
    "    \n",
    "    def get_name(self):\n",
    "        return 'sigmoid'\n",
    "    \n",
    "    ## Showing I could roll my own\n",
    "    # def sigmoid(self, X):\n",
    "    #     return 1. / (1 + numpy.exp(-X))\n",
    "    # expit is 10x faster http://stackoverflow.com/questions/21106134/numpy-pure-functions-for-performance-caching\n",
    "    def sigmoid(self, X):\n",
    "        return expit(X)\n",
    "    \n",
    "    def fprop(self, inputs):\n",
    "        a = super(Sigmoid, self).fprop(inputs)\n",
    "        return self.sigmoid(a)\n",
    "    \n",
    "    def bprop(self, h, igrads):\n",
    "        # h = Sigmoid(a) = 1. / (1 + numpy.exp(-a))\n",
    "        # dh/da = numpy.exp(-a) / (1 + numpy.exp(-a))**2\n",
    "        #       = h(1-h)\n",
    "        deltas = igrads * h * (1-h)\n",
    "        ograds = deltas.dot(self.W.T)\n",
    "        return deltas, ograds\n",
    "    \n",
    "    def bprop_cost(self, h, igrads, cost):\n",
    "        if cost is None or cost.get_name() == 'ce':\n",
    "            # for Sigmoid layer and cross entropy cost,\n",
    "            # cost back-prop is the same as standard back-prop\n",
    "            return self.bprop(h, igrads)\n",
    "        else:\n",
    "            raise NotImplementedError('Linear.bprop_cost method not implemented '\n",
    "                                      'for the %s cost' % cost.get_name())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Softmax (15%)\n",
    "\n",
    "In this task you need to create a class `Softmax` which encapsulates a layer of softmax units.  As in the previous task, you should do this by extending the `mlp.layers.Linear` class (in file `mlp/layers.py`).\n",
    "\n",
    "When you have implemented `Softmax` (in the `mlp.layers` module), then please test it by running the below code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mlp.layers import Softmax\n",
    "\n",
    "a = numpy.asarray([-20.1, 52.4, 0, 0.05, 0.05, 49])\n",
    "b = numpy.asarray([0, 0, 0, 0, 0, 0, 0, 1])\n",
    "\n",
    "rng.set_state(rng_state)\n",
    "softmax = Softmax(idim=a.shape[0], odim=b.shape[0], rng=rng)\n",
    "\n",
    "fp = softmax.fprop(a)\n",
    "deltas, ograds = softmax.bprop_cost(h=None, igrads=fp-b, cost=None)\n",
    "\n",
    "print fp.sum()\n",
    "print deltas.sum()\n",
    "print ograds.sum()\n",
    "%precision 3\n",
    "print fp\n",
    "print deltas\n",
    "print ograds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "To include the `Softmax` code in the notebook please run the below code cell.  (The notebook `%load` command is used to load the source of the `Softmax` class from `mlp/layers.py`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s Softmax mlp/layers.py\n",
    "class Softmax(Linear):\n",
    "    def get_name(self):\n",
    "        return 'softmax'\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        ## For matrices\n",
    "        ex = numpy.exp(X)\n",
    "        # Hack for a batch of size 1...must be a better way to deal with this\n",
    "        if ex.ndim == 1:\n",
    "            ex = ex[numpy.newaxis, :]\n",
    "        tot = numpy.sum(ex, axis=1, keepdims=True)\n",
    "        assert(tot.shape[0] == ex.shape[0]), \\\n",
    "            \"Total of exponents should be size N. Sum size %d, N from X is %d\" % (tot.shape[0], ex.shape[0])\n",
    "        return ex / tot\n",
    "#         ex = numpy.exp(x)\n",
    "#         tot = numpy.sum(ex)\n",
    "#         return ex / tot\n",
    "    \n",
    "    def fprop(self, inputs):\n",
    "        a = super(Softmax, self).fprop(inputs)\n",
    "        return self.softmax(a)\n",
    "    \n",
    "    # TODO: should probably NotImplementedError bprop and edit bprop_cost\n",
    "    def bprop(self, h, igrads):\n",
    "        # h = Softmax(a) = np.exp(a) / np.sum(np.exp(a))\n",
    "        # dh_c/da_k = h_c(\\dirac_ck - h_k)  #NB a matrix\n",
    "        # see end of lecture notes mlp05-hid\n",
    "        # USE AS TOP LAYER ONLY\n",
    "        ograds = numpy.dot(igrads, self.W.T)\n",
    "        return igrads, ograds\n",
    "        \n",
    "    def bprop_cost(self, h, igrads, cost):\n",
    "        if cost is None or cost.get_name() == 'ce':\n",
    "            # for softmax layer and cross entropy cost,\n",
    "            # cost back-prop is the same as standard back-prop\n",
    "            return self.bprop(h, igrads)\n",
    "        else:\n",
    "            raise NotImplementedError('Linear.bprop_cost method not implemented '\n",
    "                                      'for the %s cost' % cost.get_name())\n",
    "        return deltas, ograds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Multi-layer network for MNIST classification (40%)\n",
    "\n",
    "**(a)** (20%)  Building on the single layer linear network for MNIST classification used in lab [02_MNIST_SLN.ipynb](02_MNIST_SLN.ipynb), and using the `Sigmoid` and `Softmax` classes that you implemented in tasks 1 and 2, construct and learn a model that classifies MNIST images and:\n",
    "   * Has one hidden layer with a sigmoid transfer function and 100 units\n",
    "   * Uses a softmax output layer to discriminate between the 10 digit classes (use the `mlp.costs.CECost()` cost)\n",
    "\n",
    "Your code should print the final values of the error function and the classification accuracy for train, validation, and test sets (please keep also the log information printed by default by the optimiser). Limit the number of training epochs to 30. You can, of course, split your code across as many cells as you think is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# include here the complete code that constructs the model, performs training,\n",
    "# and prints the error and accuracy for train/valid/test\n",
    "\n",
    "import logging\n",
    "np = numpy\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from mlp.layers import MLP, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "rng = np.random.RandomState([2015,10,10])\n",
    "cost = CECost()\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=100, rng=rng))\n",
    "model.add_layer(Softmax(idim=100, odim=10, rng=rng))\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs as stopping criterion\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.01, max_epochs=30)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Initialising data providers...')\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=-1, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "cost, accuracy = optimiser.validate(model, test_dp)\n",
    "logger.info('MNIST test set accuracy is %.2f %% (cost is %.3f)'%(accuracy*100., cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** (10%) Investigate the impact of different learning rates $\\eta \\in \\{0.5, 0.2, 0.1, 0.05, 0.01, 0.005\\}$ on the convergence of the network training as well as the final accuracy:\n",
    "   * Plot (on a single graph) the error rate curves for each learning rate as a function of training epochs for training set\n",
    "   * Plot (on another single graph) the error rate curves as a function of training epochs for validation set\n",
    "   * Include a table of the corresponding error rates for test set\n",
    "\n",
    "The notebook command `%matplotlib inline` ensures that your graphs will be added to the notebook, rather than opened as additional windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning rates to test\n",
    "# ======================\n",
    "learning_rates = [0.5, 0.2, 0.1, 0.05, 0.01, 0.005]\n",
    "max_epochs = 3\n",
    "\n",
    "# Create model objects\n",
    "# ====================\n",
    "import numpy as np\n",
    "from mlp.layers import MLP, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "from copy import deepcopy\n",
    "\n",
    "rng = np.random.RandomState([2015,10,10])\n",
    "cost = CECost()\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=100, rng=rng))\n",
    "model.add_layer(Softmax(idim=100, odim=10, rng=rng))\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=-1, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "\n",
    "constantParams = {\n",
    "    'model': model,\n",
    "    'train_dp': train_dp,\n",
    "    'valid_dp': valid_dp,\n",
    "    'test_dp': test_dp\n",
    "}\n",
    "\n",
    "grid = [\n",
    "    dict(\n",
    "        deepcopy(constantParams).items() +\n",
    "        [\n",
    "            ('learning_rate', learning_rate),\n",
    "            ('lr_scheduler', LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)),\n",
    "            ('tr_stats', []),\n",
    "            ('valid_stats', []),\n",
    "            ('test_cost', None),\n",
    "            ('test_accuracy', None)\n",
    "        ]\n",
    "    )\n",
    "    for learning_rate in learning_rates\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training started with learning rate 0.500\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.576. Accuracy is 83.27%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.289. Accuracy is 91.59%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 21314 pps. Validation speed 46501 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.286. Accuracy is 91.62%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.236. Accuracy is 93.19%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 19963 pps. Validation speed 36063 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.234. Accuracy is 93.08%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.199. Accuracy is 94.41%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 17687 pps. Validation speed 47937 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 94.19 % (cost is 0.203)\n",
      "INFO:root:Training started with learning rate 0.200\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.859. Accuracy is 78.16%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.385. Accuracy is 89.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 16725 pps. Validation speed 42260 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.369. Accuracy is 89.71%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.303. Accuracy is 91.35%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 17073 pps. Validation speed 40087 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.314. Accuracy is 90.98%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.270. Accuracy is 92.23%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 20090 pps. Validation speed 47181 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 91.95 % (cost is 0.279)\n",
      "INFO:root:Training started with learning rate 0.100\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.215. Accuracy is 70.81%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.561. Accuracy is 87.57%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 20986 pps. Validation speed 42433 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.489. Accuracy is 87.61%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.384. Accuracy is 89.97%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 20245 pps. Validation speed 48002 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.387. Accuracy is 89.40%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.331. Accuracy is 90.84%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 17815 pps. Validation speed 39392 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 90.44 % (cost is 0.343)\n",
      "INFO:root:Training started with learning rate 0.050\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.662. Accuracy is 60.98%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.977. Accuracy is 80.77%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 19072 pps. Validation speed 44720 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.756. Accuracy is 83.52%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.559. Accuracy is 87.84%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 19723 pps. Validation speed 44069 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.530. Accuracy is 87.01%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.438. Accuracy is 89.21%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 20131 pps. Validation speed 43221 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 88.65 % (cost is 0.455)\n",
      "INFO:root:Training started with learning rate 0.010\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.214. Accuracy is 37.38%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.106. Accuracy is 59.70%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 19759 pps. Validation speed 41851 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.978. Accuracy is 61.22%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.818. Accuracy is 66.89%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 19229 pps. Validation speed 44285 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.660. Accuracy is 68.05%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 1.472. Accuracy is 72.08%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 19299 pps. Validation speed 43022 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 70.80 % (cost is 1.477)\n",
      "INFO:root:Training started with learning rate 0.005\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.263. Accuracy is 26.94%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.212. Accuracy is 36.31%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 18690 pps. Validation speed 43095 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.163. Accuracy is 49.40%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 2.105. Accuracy is 56.19%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 19587 pps. Validation speed 45206 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 2.048. Accuracy is 59.09%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 1.975. Accuracy is 64.26%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 20677 pps. Validation speed 37472 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 64.15 % (cost is 1.977)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ========\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for config in grid:\n",
    "    optimiser = SGDOptimiser(lr_scheduler=config['lr_scheduler'])\n",
    "    logger.info('Training started with learning rate %0.3f' % config['learning_rate'])\n",
    "    config['tr_stats'], config['valid_stats'] = \\\n",
    "        optimiser.train(config['model'], config['train_dp'], config['valid_dp'])\n",
    "    logger.info('Testing the model on test set:')\n",
    "    config['test_cost'], config['test_accuracy'] = \\\n",
    "        optimiser.validate(config['model'], config['test_dp'])\n",
    "    logger.info('MNIST test set accuracy is %.2f %% (cost is %.3f)' % \\\n",
    "                (config['test_accuracy']*100., config['test_cost']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Plot (on a single graph) the error rate curves for each learning rate as a function of training epochs for training set\n",
    "tmp = [m['tr_stats'] for m in grid]\n",
    "df = pd.DataFrame(np.array([np.array(i)[:,1] for i in tmp]).T, columns=learning_rates)\n",
    "df.plot().set_title('Training Accuracy (%)')\n",
    "\n",
    "# Plot (on another single graph) the error rate curves as a function of training epochs for validation set\n",
    "tmp = [m['valid_stats'] for m in grid]\n",
    "df = pd.DataFrame(np.array([np.array(i)[:,1] for i in tmp]).T, columns=learning_rates)\n",
    "df.plot().set_title('Validation Accuracy (%)')\n",
    "\n",
    "# Include a table of the corresponding error rates for test set\n",
    "testres = pd.DataFrame(grid)[['learning_rate', 'test_cost', 'test_accuracy']]\n",
    "testres\n",
    "\n",
    "# plt.subplot(211)\n",
    "# for learning_rate\n",
    "# plt.plot([1,2,3], label=\"test1\")\n",
    "# plt.plot([3,2,1], label=\"test2\")\n",
    "# # Place a legend above this legend, expanding itself to\n",
    "# # fully use the given bounding box.\n",
    "# plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "#            ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "# plt.subplot(223)\n",
    "# plt.plot([1,2,3], label=\"test1\")\n",
    "# plt.plot([3,2,1], label=\"test2\")\n",
    "# # Place a legend to the right of this smaller figure.\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The high training and validation accuracy increase as the learning rate decreases. However, note that the test accuracy is best for the slower learning rate. This suggests that we may be overfitting._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** (10%) Plot the following graphs:\n",
    "  * Display the 784-element weight vector of each of the 100 hidden units as 10x10 grid plot of 28x28 images, in order to visualise what features of the input they are encoding.  To do this, take the weight vector of each hidden unit, reshape to 28x28, and plot using the `imshow` function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Function `hinton` for plotting hinton diagrams was taken from the matplotlib examples_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    if not max_weight:\n",
    "        max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    for (x,y),w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w))\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigmoid_Ws = [config['model'].layers[0].W for config in grid]\n",
    "\n",
    "# So that all plots are relative, find the min and max values over all models\n",
    "max_w = np.max([(np.max(W), np.min(W)) for W in sigmoid_Ws])\n",
    "min_w = np.min([(np.max(W), np.min(W)) for W in sigmoid_Ws])\n",
    "\n",
    "for i, W in enumerate(sigmoid_Ws):\n",
    "    fig, axList = plt.subplots(10, 10)  # There are 100 weights in the layer\n",
    "    fig.set_size_inches(14, 14)\n",
    "    axList = axList.flatten()\n",
    "    for j, ax in enumerate(axList):\n",
    "        hinton(W[:,j].reshape(28,28), max_weight=max_w, ax=ax)\n",
    "    fig.suptitle(\"Model weights for learning rate %0.3f\" % learning_rates[i])\n",
    "    fig.savefig(\"/Users/kungfujam/git/mlpractical/data/figures/weights_hinton__lr%0.3f.eps\" \\\n",
    "                %learning_rates[i], dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** (cont.) (10%) Plot the following graphs:\n",
    "    * Plot a Hinton Diagram of the output layer weight matrix for digits 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------> TODO: **Plot a Hinton Diagram of the output layer weight matrix for digits 0 and 1** <------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 4 - Experiments with 1-5 hidden layers (30%)\n",
    "\n",
    "In this task use the learning rate which resulted in the best accuracy in your experiments in Task 3 (b).  Perform the following experiments:\n",
    "\n",
    "  * Train a similar model to Task 3, with one hidden layer, but with 800 hidden units. \n",
    "  * Train 4 additional models with 2, 3, 4 and 5 hidden layers.  Set the number of hidden units for each model, such that all the models have similar number of trainable weights ($\\pm$2%).   For simplicity, for a given model, keep the number of units in each hidden layer the same.\n",
    "  * Plot value of the error function for training and validation sets as a function of training epochs for each model\n",
    "  * Plot the test set classification accuracy as a function of the number of hidden layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the end of coursework 1.\n",
    "\n",
    "Please remember to save your notebook, and submit your notebook following the instructions at the top.  Please make sure that you have executed all the code cells when you submit the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
