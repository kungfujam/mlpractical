{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Admin functions\n",
    "# https://ipython.org/ipython-doc/dev/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "# When modules are changed externally, uses latest version\n",
    "%autoreload 2\n",
    "# Lanches a console for interaction with this kernel (for testing)\n",
    "%qtconsole\n",
    "# If you get an error here, try running conda install ipyparallel and running again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework #1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This coursework is concerned with building multi-layer networks to address the MNIST digit classification problem. It builds on the previous labs, in particular [02_MNIST_SLN.ipynb](02_MNIST_SLN.ipynb) in which single layer networks were trained for MNIST digit classification.   The course will involve extending that code to use Sigmoid and Softmax layers, combining these into multi-layer networks, and carrying out a number of MNIST digit classification experiments, to investigate the effect of learning rate, the number of hidden units, and the number of hidden layers.\n",
    "\n",
    "The coursework is divided into 4 tasks:\n",
    "* **Task 1**:   *Implementing a sigmoid layer* - 15 marks.  \n",
    "This task involves extending the `Linear` class in file `mlp/layers.py` to `Sigmoid`, with code for forward prop, backprop computation of the gradient, and weight update.\n",
    "* **Task 2**:  *Implementing a softmax layer* - 15 marks.  \n",
    "This task involves extending the `Linear` class in file `mlp/layers.py` to `Softmax`, with code for forward prop, backprop computation of the gradient, and weight update.\n",
    "* **Task 3**:  *Constructing a multi-layer network* - 40 marks.  \n",
    "This task involves putting together a Sigmoid and a Softmax layer to create a multi-layer network, with one hidden layer (100 units) and one output layer, that is trained to classify MNIST digits.  This task will include reporting classification results, exploring the effect of learning rates, and plotting Hinton Diagrams for the hidden units and output units.\n",
    "* **Task 4**:  *Experiments with different architectures*  - 30 marks.  \n",
    "This task involves further MNIST classification experiments, primarily looking at the effect of using different numbers of hidden layers.\n",
    "The coursework will be marked out of 100, and will contribute 30% of the total mark in the MLP course.\n",
    "\n",
    "## Previous Tutorials\n",
    "\n",
    "Before starting this coursework make sure that you have completed the first three labs:\n",
    "\n",
    "* [00_Introduction.ipynb](00_Introduction.ipynb) - setting up your environment; *Solutions*: [00_Introduction_solution.ipynb](00_Introduction_solution.ipynb)\n",
    "* [01_Linear_Models.ipynb](01_Linear_Models.ipynb) - training single layer networks; *Solutions*: [01_Linear_Models_solution.ipynb](01_Linear_Models_solution.ipynb)\n",
    "* [02_MNIST_SLN.ipynb](02_MNIST_SLN.ipynb) - training a single layer network for MNIST digit classification\n",
    "\n",
    "To ensure that your virtual environment is correct, please see [this note](https://github.com/CSTR-Edinburgh/mlpractical/blob/master/kernel_issue_fix.md) on the GitHub.\n",
    "## Submission\n",
    "**Submission Deadline:  Thursday 29 October, 16:00** \n",
    "\n",
    "Submit the coursework as an ipython notebook file, using the `submit` command in the terminal on a DICE machine. If your file is `03_MLP_Coursework1.ipynb` then you would enter:\n",
    "\n",
    "`submit mlp 1 03_MLP_Coursework1.ipynb` \n",
    "\n",
    "where `mlp 1` indicates this is the first coursework of MLP.\n",
    "\n",
    "After submitting, you should receive an email of acknowledgment from the system confirming that your submission has been received successfully. Keep the email as evidence of your coursework submission.\n",
    "\n",
    "**Please make sure you submit a single `ipynb` file (and nothing else)!**\n",
    "\n",
    "**Submission Deadline:  Thursday 29 October, 16:00** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Please enter your exam number and the date in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP Coursework 1\n",
    "#Exam number: B058714\n",
    "#Date: 20/10/2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the next code cell, which imports `numpy` and seeds the random number generator.  Please **do not** modify the random number generator seed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "#Seed a random number generator running the below cell, but do **not** modify the seed.\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "rng_state = rng.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Sigmoid Layer (15%)\n",
    "\n",
    "In this task you need to create a class `Sigmoid` which encapsulates a layer of sigmoid units.  You should do this by extending the `mlp.layers.Linear` class (in file `mlp/layers.py`), which implements a layer of linear units (i.e. weighted sum plus bias).  The `Sigmoid` class extends this by applying the sigmoid transfer function to the weighted sum in the forward propagation, and applying the derivative of the sigmoid in the gradient descent back propagation and computing the gradients with respect to layer's parameters. Do **not** copy the implementation provided in `Linear` class but rather, **reuse** it through inheritance.\n",
    "\n",
    "When you have implemented `Sigmoid` (in the `mlp.layers` module), then please test it by running the below code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.21941537867\n",
      "25.9255147706\n",
      "4.1105500626\n",
      "[ 0.067  0.728  0.999  0.512  0.159  0.584  0.238  0.932]\n",
      "[ -1.263e+00   1.037e+01   0.000e+00   1.249e-02   6.678e-03   1.191e+01\n",
      "   3.628e+00   1.268e+00]\n",
      "[ 1.406  0.078 -0.268  0.418  1.646  0.831]\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import Sigmoid\n",
    "\n",
    "a = numpy.asarray([-20.1, 52.4, 0, 0.05, 0.05, 49])\n",
    "b = numpy.asarray([-20.1, 52.4, 0, 0.05, 0.05, 49, 20, 20])\n",
    "\n",
    "rng.set_state(rng_state)\n",
    "sigm = Sigmoid(idim=a.shape[0], odim=b.shape[0], rng=rng)\n",
    "\n",
    "fp = sigm.fprop(a)\n",
    "deltas, ograds  = sigm.bprop(h=fp, igrads=b)\n",
    "\n",
    "print fp.sum()\n",
    "print deltas.sum()\n",
    "print ograds.sum()\n",
    "%precision 3\n",
    "print fp\n",
    "print deltas\n",
    "print ograds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "To include the `Sigmoid` code in the notebook please run the below code cell.  (The `%load` notebook command is used to load the source of the `Sigmoid` class from `mlp/layers.py`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load -s Sigmoid mlp/layers.py\n",
    "class Sigmoid(Linear):\n",
    "    \n",
    "    def get_name(self):\n",
    "        return 'sigmoid'\n",
    "    \n",
    "    ## Showing I could roll my own\n",
    "    # def sigmoid(self, X):\n",
    "    #     return 1. / (1 + numpy.exp(-X))\n",
    "    # expit is 10x faster http://stackoverflow.com/questions/21106134/numpy-pure-functions-for-performance-caching\n",
    "    def sigmoid(self, X):\n",
    "        return expit(X)\n",
    "    \n",
    "    def fprop(self, inputs):\n",
    "        a = super(Sigmoid, self).fprop(inputs)\n",
    "        return self.sigmoid(a)\n",
    "    \n",
    "    def bprop(self, h, igrads):\n",
    "        # h = Sigmoid(a) = 1. / (1 + numpy.exp(-a))\n",
    "        # dh/da = numpy.exp(-a) / (1 + numpy.exp(-a))**2\n",
    "        #       = h(1-h)\n",
    "        deltas = igrads * h * (1-h)\n",
    "        ograds = deltas.dot(self.W.T)\n",
    "        return deltas, ograds\n",
    "    \n",
    "    def bprop_cost(self, h, igrads, cost):\n",
    "        if cost is None or cost.get_name() == 'ce':\n",
    "            # for Sigmoid layer and cross entropy cost,\n",
    "            # cost back-prop is the same as standard back-prop\n",
    "            return self.bprop(h, igrads)\n",
    "        else:\n",
    "            raise NotImplementedError('Linear.bprop_cost method not implemented '\n",
    "                                      'for the %s cost' % cost.get_name())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Softmax (15%)\n",
    "\n",
    "In this task you need to create a class `Softmax` which encapsulates a layer of softmax units.  As in the previous task, you should do this by extending the `mlp.layers.Linear` class (in file `mlp/layers.py`).\n",
    "\n",
    "When you have implemented `Softmax` (in the `mlp.layers` module), then please test it by running the below code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.0744177068753\n",
      "[[  4.571e-05   1.697e-03   9.877e-01   6.631e-04   1.194e-04   8.880e-04\n",
      "    1.977e-04   8.671e-03]]\n",
      "[[  4.571e-05   1.697e-03   9.877e-01   6.631e-04   1.194e-04   8.880e-04\n",
      "    1.977e-04  -9.913e-01]]\n",
      "[[-0.089  0.03   0.079  0.011  0.017  0.027]]\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import Softmax\n",
    "\n",
    "a = numpy.asarray([-20.1, 52.4, 0, 0.05, 0.05, 49])\n",
    "b = numpy.asarray([0, 0, 0, 0, 0, 0, 0, 1])\n",
    "\n",
    "rng.set_state(rng_state)\n",
    "softmax = Softmax(idim=a.shape[0], odim=b.shape[0], rng=rng)\n",
    "\n",
    "fp = softmax.fprop(a)\n",
    "deltas, ograds = softmax.bprop_cost(h=None, igrads=fp-b, cost=None)\n",
    "\n",
    "print fp.sum()\n",
    "print deltas.sum()\n",
    "print ograds.sum()\n",
    "%precision 3\n",
    "print fp\n",
    "print deltas\n",
    "print ograds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "To include the `Softmax` code in the notebook please run the below code cell.  (The notebook `%load` command is used to load the source of the `Softmax` class from `mlp/layers.py`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s Softmax mlp/layers.py\n",
    "class Softmax(Linear):\n",
    "    def get_name(self):\n",
    "        return 'softmax'\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        ## For matrices\n",
    "        ex = numpy.exp(X)\n",
    "        # Hack for a batch of size 1...must be a better way to deal with this\n",
    "        if ex.ndim == 1:\n",
    "            ex = ex[numpy.newaxis, :]\n",
    "        tot = numpy.sum(ex, axis=1, keepdims=True)\n",
    "        assert(tot.shape[0] == ex.shape[0]), \\\n",
    "            \"Total of exponents should be size N. Sum size %d, N from X is %d\" % (tot.shape[0], ex.shape[0])\n",
    "        return ex / tot\n",
    "#         ex = numpy.exp(x)\n",
    "#         tot = numpy.sum(ex)\n",
    "#         return ex / tot\n",
    "    \n",
    "    def fprop(self, inputs):\n",
    "        a = super(Softmax, self).fprop(inputs)\n",
    "        return self.softmax(a)\n",
    "    \n",
    "    # TODO: should probably NotImplementedError bprop and edit bprop_cost\n",
    "    def bprop(self, h, igrads):\n",
    "        # h = Softmax(a) = np.exp(a) / np.sum(np.exp(a))\n",
    "        # dh_c/da_k = h_c(\\dirac_ck - h_k)  #NB a matrix\n",
    "        # see end of lecture notes mlp05-hid\n",
    "        # USE AS TOP LAYER ONLY\n",
    "        ograds = numpy.dot(igrads, self.W.T)\n",
    "        return igrads, ograds\n",
    "        \n",
    "    def bprop_cost(self, h, igrads, cost):\n",
    "        if cost is None or cost.get_name() == 'ce':\n",
    "            # for softmax layer and cross entropy cost,\n",
    "            # cost back-prop is the same as standard back-prop\n",
    "            return self.bprop(h, igrads)\n",
    "        else:\n",
    "            raise NotImplementedError('Linear.bprop_cost method not implemented '\n",
    "                                      'for the %s cost' % cost.get_name())\n",
    "        return deltas, ograds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Multi-layer network for MNIST classification (40%)\n",
    "\n",
    "**(a)** (20%)  Building on the single layer linear network for MNIST classification used in lab [02_MNIST_SLN.ipynb](02_MNIST_SLN.ipynb), and using the `Sigmoid` and `Softmax` classes that you implemented in tasks 1 and 2, construct and learn a model that classifies MNIST images and:\n",
    "   * Has one hidden layer with a sigmoid transfer function and 100 units\n",
    "   * Uses a softmax output layer to discriminate between the 10 digit classes (use the `mlp.costs.CECost()` cost)\n",
    "\n",
    "Your code should print the final values of the error function and the classification accuracy for train, validation, and test sets (please keep also the log information printed by default by the optimiser). Limit the number of training epochs to 30. You can, of course, split your code across as many cells as you think is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.214. Accuracy is 37.87%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.106. Accuracy is 57.72%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 19301 pps. Validation speed 37438 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.978. Accuracy is 61.20%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.817. Accuracy is 66.45%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 18959 pps. Validation speed 35206 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.660. Accuracy is 68.08%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 1.472. Accuracy is 73.55%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 19400 pps. Validation speed 43575 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.343. Accuracy is 73.62%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 1.180. Accuracy is 78.86%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 3 seconds. Training speed 17994 pps. Validation speed 40145 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.102. Accuracy is 78.06%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.974. Accuracy is 82.45%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 3 seconds. Training speed 20422 pps. Validation speed 48028 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.934. Accuracy is 80.87%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.831. Accuracy is 84.44%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 3 seconds. Training speed 20866 pps. Validation speed 50025 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.817. Accuracy is 82.73%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.731. Accuracy is 85.54%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 3 seconds. Training speed 17831 pps. Validation speed 34618 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.732. Accuracy is 84.03%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.657. Accuracy is 86.64%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 3 seconds. Training speed 15534 pps. Validation speed 43455 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.669. Accuracy is 85.01%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.602. Accuracy is 87.30%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 3 seconds. Training speed 19822 pps. Validation speed 40759 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.620. Accuracy is 85.74%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.559. Accuracy is 88.03%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 3 seconds. Training speed 18694 pps. Validation speed 35971 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.582. Accuracy is 86.39%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.525. Accuracy is 88.36%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 3 seconds. Training speed 19578 pps. Validation speed 44360 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.551. Accuracy is 86.85%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.497. Accuracy is 88.58%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 3 seconds. Training speed 19469 pps. Validation speed 36287 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.525. Accuracy is 87.25%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.474. Accuracy is 88.79%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 4 seconds. Training speed 15079 pps. Validation speed 45065 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.503. Accuracy is 87.49%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.454. Accuracy is 89.03%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 3 seconds. Training speed 19410 pps. Validation speed 47499 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.484. Accuracy is 87.77%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.438. Accuracy is 89.34%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 3 seconds. Training speed 17399 pps. Validation speed 42494 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.468. Accuracy is 88.09%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.424. Accuracy is 89.37%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 3 seconds. Training speed 19445 pps. Validation speed 48501 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.454. Accuracy is 88.31%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.411. Accuracy is 89.68%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 3 seconds. Training speed 16711 pps. Validation speed 48728 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.442. Accuracy is 88.46%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.401. Accuracy is 89.65%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 3 seconds. Training speed 20079 pps. Validation speed 47442 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.431. Accuracy is 88.66%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.391. Accuracy is 89.90%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 3 seconds. Training speed 21245 pps. Validation speed 50524 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.422. Accuracy is 88.86%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.382. Accuracy is 90.05%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 3 seconds. Training speed 21719 pps. Validation speed 46541 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.413. Accuracy is 88.96%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.374. Accuracy is 90.25%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 3 seconds. Training speed 20412 pps. Validation speed 42443 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.405. Accuracy is 89.15%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.368. Accuracy is 90.37%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 3 seconds. Training speed 19942 pps. Validation speed 38867 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.398. Accuracy is 89.30%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.361. Accuracy is 90.42%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 3 seconds. Training speed 21129 pps. Validation speed 47286 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.391. Accuracy is 89.39%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.355. Accuracy is 90.49%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 3 seconds. Training speed 20247 pps. Validation speed 43793 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.385. Accuracy is 89.53%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.350. Accuracy is 90.58%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 3 seconds. Training speed 20919 pps. Validation speed 42998 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.380. Accuracy is 89.60%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.345. Accuracy is 90.64%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 3 seconds. Training speed 20539 pps. Validation speed 46758 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.375. Accuracy is 89.71%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.341. Accuracy is 90.64%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 3 seconds. Training speed 17146 pps. Validation speed 36259 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.370. Accuracy is 89.82%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.336. Accuracy is 90.80%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 3 seconds. Training speed 18246 pps. Validation speed 39924 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.365. Accuracy is 89.96%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.332. Accuracy is 90.90%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 4 seconds. Training speed 14330 pps. Validation speed 36901 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.361. Accuracy is 90.03%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.329. Accuracy is 90.86%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 3 seconds. Training speed 20304 pps. Validation speed 46879 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 90.56 % (cost is 0.340)\n"
     ]
    }
   ],
   "source": [
    "# include here the complete code that constructs the model, performs training,\n",
    "# and prints the error and accuracy for train/valid/test\n",
    "\n",
    "import logging\n",
    "np = numpy\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from mlp.layers import MLP, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "rng = np.random.RandomState([2015,10,10])\n",
    "cost = CECost()\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=100, rng=rng))\n",
    "model.add_layer(Softmax(idim=100, odim=10, rng=rng))\n",
    "# define the optimiser, here stochasitc gradient descent\n",
    "# with fixed learning rate and max_epochs as stopping criterion\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.01, max_epochs=30)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "logger.info('Initialising data providers...')\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=-1, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "\n",
    "logger.info('Training started...')\n",
    "tr_stats, valid_stats = optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "cost, accuracy = optimiser.validate(model, test_dp)\n",
    "logger.info('MNIST test set accuracy is %.2f %% (cost is %.3f)'%(accuracy*100., cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** (10%) Investigate the impact of different learning rates $\\eta \\in \\{0.5, 0.2, 0.1, 0.05, 0.01, 0.005\\}$ on the convergence of the network training as well as the final accuracy:\n",
    "   * Plot (on a single graph) the error rate curves for each learning rate as a function of training epochs for training set\n",
    "   * Plot (on another single graph) the error rate curves as a function of training epochs for validation set\n",
    "   * Include a table of the corresponding error rates for test set\n",
    "\n",
    "The notebook command `%matplotlib inline` ensures that your graphs will be added to the notebook, rather than opened as additional windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning rates to test\n",
    "# ======================\n",
    "learning_rates = [0.5, 0.2, 0.1, 0.05, 0.01, 0.005]\n",
    "max_epochs = 30\n",
    "\n",
    "# Create model objects\n",
    "# ====================\n",
    "import numpy as np\n",
    "from mlp.layers import MLP, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "from copy import deepcopy\n",
    "\n",
    "rng = np.random.RandomState([2015,10,10])\n",
    "cost = CECost()\n",
    "model = MLP(cost=cost)\n",
    "model.add_layer(Sigmoid(idim=784, odim=100, rng=rng))\n",
    "model.add_layer(Softmax(idim=100, odim=10, rng=rng))\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=-1, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "\n",
    "constantParams = {\n",
    "    'model': model,\n",
    "    'train_dp': train_dp,\n",
    "    'valid_dp': valid_dp,\n",
    "    'test_dp': test_dp\n",
    "}\n",
    "\n",
    "grid = [\n",
    "    dict(\n",
    "        deepcopy(constantParams).items() +\n",
    "        [\n",
    "            ('learning_rate', learning_rate),\n",
    "            ('lr_scheduler', LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)),\n",
    "            ('tr_stats', []),\n",
    "            ('valid_stats', []),\n",
    "            ('test_cost', None),\n",
    "            ('test_accuracy', None)\n",
    "        ]\n",
    "    )\n",
    "    for learning_rate in learning_rates\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training started with learning rate 0.500\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.576. Accuracy is 83.47%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.287. Accuracy is 91.76%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 2 seconds. Training speed 22159 pps. Validation speed 47303 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.287. Accuracy is 91.65%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.240. Accuracy is 93.19%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 2 seconds. Training speed 22224 pps. Validation speed 46430 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.235. Accuracy is 93.16%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.204. Accuracy is 94.29%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 2 seconds. Training speed 22362 pps. Validation speed 47923 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.200. Accuracy is 94.27%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.176. Accuracy is 95.02%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 2 seconds. Training speed 22099 pps. Validation speed 45439 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.174. Accuracy is 94.91%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.158. Accuracy is 95.60%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 3 seconds. Training speed 21457 pps. Validation speed 46710 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.154. Accuracy is 95.51%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.144. Accuracy is 96.06%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 2 seconds. Training speed 22005 pps. Validation speed 47744 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.137. Accuracy is 96.00%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.134. Accuracy is 96.33%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 2 seconds. Training speed 22301 pps. Validation speed 48092 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.124. Accuracy is 96.45%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.123. Accuracy is 96.72%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 2 seconds. Training speed 22286 pps. Validation speed 46779 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.114. Accuracy is 96.66%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.118. Accuracy is 96.78%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 2 seconds. Training speed 21909 pps. Validation speed 46254 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.105. Accuracy is 97.02%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.112. Accuracy is 96.92%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 3 seconds. Training speed 21628 pps. Validation speed 48652 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.097. Accuracy is 97.24%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.108. Accuracy is 97.04%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 2 seconds. Training speed 22009 pps. Validation speed 45416 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.090. Accuracy is 97.47%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.108. Accuracy is 97.02%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 3 seconds. Training speed 21874 pps. Validation speed 45823 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.084. Accuracy is 97.62%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.104. Accuracy is 97.06%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 3 seconds. Training speed 21572 pps. Validation speed 48759 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.079. Accuracy is 97.81%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.098. Accuracy is 97.25%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 3 seconds. Training speed 21337 pps. Validation speed 46545 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.074. Accuracy is 97.94%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.094. Accuracy is 97.37%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 3 seconds. Training speed 21249 pps. Validation speed 48264 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.069. Accuracy is 98.09%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.093. Accuracy is 97.24%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 3 seconds. Training speed 21691 pps. Validation speed 45333 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.066. Accuracy is 98.17%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.092. Accuracy is 97.29%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 3 seconds. Training speed 21497 pps. Validation speed 45157 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.062. Accuracy is 98.30%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.092. Accuracy is 97.25%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 3 seconds. Training speed 21500 pps. Validation speed 44020 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.058. Accuracy is 98.47%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.089. Accuracy is 97.44%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 3 seconds. Training speed 21529 pps. Validation speed 46277 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.056. Accuracy is 98.50%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.088. Accuracy is 97.40%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 3 seconds. Training speed 20959 pps. Validation speed 44704 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.053. Accuracy is 98.60%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.086. Accuracy is 97.39%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 3 seconds. Training speed 21045 pps. Validation speed 45439 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.050. Accuracy is 98.69%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.084. Accuracy is 97.49%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 3 seconds. Training speed 20586 pps. Validation speed 43628 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.048. Accuracy is 98.78%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.084. Accuracy is 97.53%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 3 seconds. Training speed 21493 pps. Validation speed 44822 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.045. Accuracy is 98.86%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.082. Accuracy is 97.46%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 3 seconds. Training speed 21304 pps. Validation speed 48364 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.043. Accuracy is 98.90%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.082. Accuracy is 97.50%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 3 seconds. Training speed 21318 pps. Validation speed 46431 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.041. Accuracy is 99.00%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.083. Accuracy is 97.47%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 3 seconds. Training speed 21617 pps. Validation speed 44799 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.039. Accuracy is 99.06%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.080. Accuracy is 97.55%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 3 seconds. Training speed 21229 pps. Validation speed 44392 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.037. Accuracy is 99.11%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.081. Accuracy is 97.60%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 3 seconds. Training speed 21402 pps. Validation speed 48027 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.036. Accuracy is 99.18%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.080. Accuracy is 97.49%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 3 seconds. Training speed 21529 pps. Validation speed 46106 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.034. Accuracy is 99.25%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.079. Accuracy is 97.55%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 3 seconds. Training speed 17171 pps. Validation speed 36308 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.68 % (cost is 0.077)\n",
      "INFO:root:Training started with learning rate 0.200\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 0.858. Accuracy is 77.95%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.385. Accuracy is 90.15%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 16128 pps. Validation speed 45216 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.370. Accuracy is 89.67%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.306. Accuracy is 91.49%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 20195 pps. Validation speed 45445 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.314. Accuracy is 91.03%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.275. Accuracy is 92.19%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 20980 pps. Validation speed 44966 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.284. Accuracy is 91.83%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.250. Accuracy is 92.92%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 3 seconds. Training speed 20770 pps. Validation speed 44264 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.261. Accuracy is 92.46%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.235. Accuracy is 93.37%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 3 seconds. Training speed 20648 pps. Validation speed 47324 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.241. Accuracy is 93.04%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.218. Accuracy is 93.77%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 3 seconds. Training speed 19156 pps. Validation speed 46833 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.225. Accuracy is 93.55%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.206. Accuracy is 94.30%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 3 seconds. Training speed 21348 pps. Validation speed 45878 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.210. Accuracy is 93.98%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.194. Accuracy is 94.69%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 3 seconds. Training speed 21274 pps. Validation speed 47549 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.197. Accuracy is 94.25%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.187. Accuracy is 94.78%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 3 seconds. Training speed 20946 pps. Validation speed 44755 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.186. Accuracy is 94.64%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.176. Accuracy is 95.22%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 3 seconds. Training speed 21253 pps. Validation speed 53804 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.175. Accuracy is 94.90%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.169. Accuracy is 95.42%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 3 seconds. Training speed 19469 pps. Validation speed 39580 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.166. Accuracy is 95.16%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.160. Accuracy is 95.63%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 3 seconds. Training speed 18180 pps. Validation speed 48603 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.157. Accuracy is 95.49%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.158. Accuracy is 95.86%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 3 seconds. Training speed 20389 pps. Validation speed 52988 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.150. Accuracy is 95.67%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.149. Accuracy is 95.89%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 3 seconds. Training speed 20360 pps. Validation speed 39268 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.143. Accuracy is 95.90%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.143. Accuracy is 96.17%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 3 seconds. Training speed 19920 pps. Validation speed 51599 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.136. Accuracy is 96.10%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.139. Accuracy is 96.21%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 3 seconds. Training speed 21024 pps. Validation speed 52977 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.130. Accuracy is 96.28%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.135. Accuracy is 96.26%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 3 seconds. Training speed 20949 pps. Validation speed 49071 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.125. Accuracy is 96.42%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.131. Accuracy is 96.40%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 3 seconds. Training speed 21262 pps. Validation speed 52995 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.120. Accuracy is 96.59%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.128. Accuracy is 96.49%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 3 seconds. Training speed 20900 pps. Validation speed 52805 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.116. Accuracy is 96.71%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.125. Accuracy is 96.67%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 3 seconds. Training speed 21381 pps. Validation speed 54616 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.112. Accuracy is 96.86%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.121. Accuracy is 96.68%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 3 seconds. Training speed 21420 pps. Validation speed 53905 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.108. Accuracy is 97.00%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.120. Accuracy is 96.71%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 3 seconds. Training speed 20990 pps. Validation speed 53813 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.104. Accuracy is 97.12%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.118. Accuracy is 96.79%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 3 seconds. Training speed 20779 pps. Validation speed 53188 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.101. Accuracy is 97.21%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.116. Accuracy is 96.79%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 3 seconds. Training speed 20995 pps. Validation speed 53890 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.097. Accuracy is 97.33%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.113. Accuracy is 96.84%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 2 seconds. Training speed 21648 pps. Validation speed 54122 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.094. Accuracy is 97.43%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.110. Accuracy is 96.92%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 3 seconds. Training speed 21584 pps. Validation speed 54460 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.091. Accuracy is 97.48%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.109. Accuracy is 96.98%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 3 seconds. Training speed 21322 pps. Validation speed 52380 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.088. Accuracy is 97.55%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.107. Accuracy is 96.98%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 3 seconds. Training speed 21579 pps. Validation speed 54559 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.086. Accuracy is 97.66%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.105. Accuracy is 97.22%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 3 seconds. Training speed 21418 pps. Validation speed 54458 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.083. Accuracy is 97.72%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.103. Accuracy is 97.17%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 3 seconds. Training speed 21749 pps. Validation speed 48791 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 97.04 % (cost is 0.101)\n",
      "INFO:root:Training started with learning rate 0.100\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.214. Accuracy is 70.96%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.564. Accuracy is 87.29%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 2 seconds. Training speed 22028 pps. Validation speed 53525 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.490. Accuracy is 87.58%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.386. Accuracy is 90.05%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 2 seconds. Training speed 21942 pps. Validation speed 53520 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.387. Accuracy is 89.42%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.330. Accuracy is 90.75%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 2 seconds. Training speed 21773 pps. Validation speed 53818 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.345. Accuracy is 90.29%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.303. Accuracy is 91.37%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 3 seconds. Training speed 21648 pps. Validation speed 51347 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.320. Accuracy is 90.86%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.285. Accuracy is 91.78%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 4 seconds. Training speed 14891 pps. Validation speed 47631 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.302. Accuracy is 91.32%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.272. Accuracy is 92.20%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 3 seconds. Training speed 15564 pps. Validation speed 42126 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.287. Accuracy is 91.74%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.259. Accuracy is 92.56%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 3 seconds. Training speed 15660 pps. Validation speed 41016 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.275. Accuracy is 92.13%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.249. Accuracy is 92.79%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 3 seconds. Training speed 16738 pps. Validation speed 49992 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.263. Accuracy is 92.37%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.241. Accuracy is 93.09%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 3 seconds. Training speed 21410 pps. Validation speed 49317 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.253. Accuracy is 92.71%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.232. Accuracy is 93.33%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 3 seconds. Training speed 21356 pps. Validation speed 48124 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.244. Accuracy is 92.98%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.227. Accuracy is 93.62%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 3 seconds. Training speed 21747 pps. Validation speed 48350 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.235. Accuracy is 93.33%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.219. Accuracy is 93.86%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 3 seconds. Training speed 21505 pps. Validation speed 46791 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.226. Accuracy is 93.55%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.212. Accuracy is 94.03%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 3 seconds. Training speed 21439 pps. Validation speed 48504 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.219. Accuracy is 93.75%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.206. Accuracy is 94.26%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 3 seconds. Training speed 21687 pps. Validation speed 47713 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.211. Accuracy is 93.96%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.200. Accuracy is 94.56%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 3 seconds. Training speed 21578 pps. Validation speed 48191 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.205. Accuracy is 94.13%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.195. Accuracy is 94.67%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 3 seconds. Training speed 21584 pps. Validation speed 48527 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.198. Accuracy is 94.29%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.189. Accuracy is 94.93%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 3 seconds. Training speed 21684 pps. Validation speed 48959 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.192. Accuracy is 94.49%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.184. Accuracy is 95.07%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 3 seconds. Training speed 21521 pps. Validation speed 47286 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.186. Accuracy is 94.64%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.181. Accuracy is 95.10%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 3 seconds. Training speed 21555 pps. Validation speed 48961 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.181. Accuracy is 94.80%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.175. Accuracy is 95.25%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 3 seconds. Training speed 21266 pps. Validation speed 48413 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.176. Accuracy is 94.98%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.172. Accuracy is 95.31%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 3 seconds. Training speed 21419 pps. Validation speed 48688 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.171. Accuracy is 95.10%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.168. Accuracy is 95.57%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 2 seconds. Training speed 21825 pps. Validation speed 50104 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.166. Accuracy is 95.16%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.164. Accuracy is 95.65%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 2 seconds. Training speed 21992 pps. Validation speed 49659 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.162. Accuracy is 95.36%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.161. Accuracy is 95.60%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 2 seconds. Training speed 21805 pps. Validation speed 48875 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.158. Accuracy is 95.41%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.158. Accuracy is 95.73%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 3 seconds. Training speed 21552 pps. Validation speed 46253 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.154. Accuracy is 95.57%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.156. Accuracy is 95.88%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 3 seconds. Training speed 21719 pps. Validation speed 49842 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.150. Accuracy is 95.68%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.152. Accuracy is 95.89%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 2 seconds. Training speed 21897 pps. Validation speed 48116 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.146. Accuracy is 95.80%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.149. Accuracy is 95.99%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 3 seconds. Training speed 21068 pps. Validation speed 48635 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.143. Accuracy is 95.87%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.147. Accuracy is 96.00%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 2 seconds. Training speed 21853 pps. Validation speed 49875 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.140. Accuracy is 95.95%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.145. Accuracy is 96.08%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 3 seconds. Training speed 21808 pps. Validation speed 48060 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 95.77 % (cost is 0.144)\n",
      "INFO:root:Training started with learning rate 0.050\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.663. Accuracy is 61.43%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 0.976. Accuracy is 82.99%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 21743 pps. Validation speed 48903 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 0.757. Accuracy is 83.32%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.564. Accuracy is 87.77%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 21636 pps. Validation speed 49216 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.530. Accuracy is 87.10%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.438. Accuracy is 89.41%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 21404 pps. Validation speed 48047 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.445. Accuracy is 88.41%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.382. Accuracy is 90.02%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 4 seconds. Training speed 15451 pps. Validation speed 37918 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.400. Accuracy is 89.16%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.351. Accuracy is 90.64%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 3 seconds. Training speed 15963 pps. Validation speed 34958 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.371. Accuracy is 89.72%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.329. Accuracy is 91.02%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 4 seconds. Training speed 13847 pps. Validation speed 33066 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.351. Accuracy is 90.14%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.314. Accuracy is 91.21%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 4 seconds. Training speed 14355 pps. Validation speed 36282 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.336. Accuracy is 90.50%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.302. Accuracy is 91.45%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 3 seconds. Training speed 18040 pps. Validation speed 40847 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.324. Accuracy is 90.80%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.293. Accuracy is 91.62%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 3 seconds. Training speed 17370 pps. Validation speed 51157 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.313. Accuracy is 91.05%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.283. Accuracy is 91.87%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 3 seconds. Training speed 19222 pps. Validation speed 46027 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.304. Accuracy is 91.26%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.277. Accuracy is 92.00%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 3 seconds. Training speed 17420 pps. Validation speed 38240 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.296. Accuracy is 91.53%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.271. Accuracy is 92.19%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 3 seconds. Training speed 17629 pps. Validation speed 34461 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.289. Accuracy is 91.72%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.264. Accuracy is 92.52%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 3 seconds. Training speed 16961 pps. Validation speed 46346 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.282. Accuracy is 91.89%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.259. Accuracy is 92.57%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 3 seconds. Training speed 17509 pps. Validation speed 34487 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.276. Accuracy is 92.09%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.254. Accuracy is 92.82%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 3 seconds. Training speed 18784 pps. Validation speed 53039 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.270. Accuracy is 92.22%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.249. Accuracy is 92.87%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 3 seconds. Training speed 19498 pps. Validation speed 43613 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.265. Accuracy is 92.37%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.245. Accuracy is 93.06%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 3 seconds. Training speed 19310 pps. Validation speed 49092 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.259. Accuracy is 92.53%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.240. Accuracy is 93.17%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 3 seconds. Training speed 15719 pps. Validation speed 46548 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.254. Accuracy is 92.69%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.236. Accuracy is 93.35%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 3 seconds. Training speed 19312 pps. Validation speed 45291 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.249. Accuracy is 92.81%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.232. Accuracy is 93.40%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 3 seconds. Training speed 19834 pps. Validation speed 50642 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.245. Accuracy is 93.01%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.228. Accuracy is 93.51%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 3 seconds. Training speed 20401 pps. Validation speed 53173 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.240. Accuracy is 93.15%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.224. Accuracy is 93.58%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 3 seconds. Training speed 20107 pps. Validation speed 53001 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.236. Accuracy is 93.29%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.222. Accuracy is 93.72%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 3 seconds. Training speed 20604 pps. Validation speed 51152 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.231. Accuracy is 93.37%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.218. Accuracy is 93.83%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 3 seconds. Training speed 19759 pps. Validation speed 51303 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.227. Accuracy is 93.53%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.215. Accuracy is 93.96%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 3 seconds. Training speed 19110 pps. Validation speed 48532 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.223. Accuracy is 93.60%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.211. Accuracy is 94.14%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 3 seconds. Training speed 18032 pps. Validation speed 47789 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.219. Accuracy is 93.68%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.208. Accuracy is 94.20%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 3 seconds. Training speed 18853 pps. Validation speed 39131 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.216. Accuracy is 93.84%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.205. Accuracy is 94.29%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 3 seconds. Training speed 17470 pps. Validation speed 46482 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.212. Accuracy is 93.92%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.202. Accuracy is 94.39%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 3 seconds. Training speed 18207 pps. Validation speed 49622 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.209. Accuracy is 94.01%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.199. Accuracy is 94.52%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 3 seconds. Training speed 20856 pps. Validation speed 52841 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 94.07 % (cost is 0.203)\n",
      "INFO:root:Training started with learning rate 0.010\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.214. Accuracy is 37.83%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.106. Accuracy is 54.44%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 18749 pps. Validation speed 47432 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.978. Accuracy is 60.68%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 1.818. Accuracy is 68.50%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 3 seconds. Training speed 20456 pps. Validation speed 51282 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 1.659. Accuracy is 68.27%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 1.472. Accuracy is 72.62%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 21035 pps. Validation speed 48928 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.343. Accuracy is 73.41%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 1.181. Accuracy is 78.98%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 3 seconds. Training speed 20398 pps. Validation speed 35902 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.102. Accuracy is 78.01%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.973. Accuracy is 82.39%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 3 seconds. Training speed 16494 pps. Validation speed 31505 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.934. Accuracy is 80.86%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.831. Accuracy is 84.02%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 4 seconds. Training speed 14833 pps. Validation speed 54499 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.817. Accuracy is 82.72%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.730. Accuracy is 85.80%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 3 seconds. Training speed 19991 pps. Validation speed 35392 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.732. Accuracy is 84.08%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.657. Accuracy is 86.57%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 3 seconds. Training speed 17291 pps. Validation speed 49884 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.669. Accuracy is 84.96%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.602. Accuracy is 87.25%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 3 seconds. Training speed 20512 pps. Validation speed 51717 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.621. Accuracy is 85.78%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.559. Accuracy is 87.94%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 3 seconds. Training speed 20845 pps. Validation speed 50605 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.582. Accuracy is 86.31%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.525. Accuracy is 88.25%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 3 seconds. Training speed 20220 pps. Validation speed 51297 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.551. Accuracy is 86.83%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.497. Accuracy is 88.66%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 3 seconds. Training speed 21667 pps. Validation speed 51849 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.525. Accuracy is 87.17%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.474. Accuracy is 88.89%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 3 seconds. Training speed 20933 pps. Validation speed 54349 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.503. Accuracy is 87.50%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.455. Accuracy is 89.04%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 3 seconds. Training speed 21544 pps. Validation speed 53309 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.484. Accuracy is 87.77%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.438. Accuracy is 89.35%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 3 seconds. Training speed 21316 pps. Validation speed 47769 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.468. Accuracy is 88.08%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.423. Accuracy is 89.59%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 3 seconds. Training speed 21390 pps. Validation speed 53032 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.454. Accuracy is 88.27%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.411. Accuracy is 89.70%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 2 seconds. Training speed 22073 pps. Validation speed 54216 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.442. Accuracy is 88.53%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.400. Accuracy is 89.79%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 3 seconds. Training speed 19486 pps. Validation speed 39214 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.431. Accuracy is 88.68%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.391. Accuracy is 89.93%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 3 seconds. Training speed 21000 pps. Validation speed 51746 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.422. Accuracy is 88.86%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.382. Accuracy is 90.20%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 4 seconds. Training speed 14804 pps. Validation speed 47142 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.413. Accuracy is 88.99%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.375. Accuracy is 90.19%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 4 seconds. Training speed 14097 pps. Validation speed 37084 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.405. Accuracy is 89.07%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.367. Accuracy is 90.32%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 3 seconds. Training speed 18538 pps. Validation speed 54737 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.398. Accuracy is 89.26%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.361. Accuracy is 90.41%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 3 seconds. Training speed 19248 pps. Validation speed 51532 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.391. Accuracy is 89.39%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.355. Accuracy is 90.47%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 3 seconds. Training speed 20450 pps. Validation speed 52092 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.385. Accuracy is 89.51%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.350. Accuracy is 90.57%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 3 seconds. Training speed 21385 pps. Validation speed 54089 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.380. Accuracy is 89.60%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.345. Accuracy is 90.57%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 3 seconds. Training speed 20691 pps. Validation speed 47079 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.374. Accuracy is 89.72%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.341. Accuracy is 90.78%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 5 seconds. Training speed 12457 pps. Validation speed 15672 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.370. Accuracy is 89.81%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.336. Accuracy is 90.78%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 6 seconds. Training speed 9204 pps. Validation speed 46623 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.365. Accuracy is 89.91%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.332. Accuracy is 90.89%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 3 seconds. Training speed 18076 pps. Validation speed 47213 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.361. Accuracy is 90.01%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.329. Accuracy is 90.94%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 3 seconds. Training speed 21060 pps. Validation speed 51459 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 90.57 % (cost is 0.340)\n",
      "INFO:root:Training started with learning rate 0.005\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for random model is 2.344. Accuracy is 12.43%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for random model is 2.344. Accuracy is 12.92%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 2.263. Accuracy is 26.96%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 2.212. Accuracy is 45.79%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 3 seconds. Training speed 19715 pps. Validation speed 48146 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 2.163. Accuracy is 50.30%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 2.106. Accuracy is 57.49%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 7 seconds. Training speed 7842 pps. Validation speed 44345 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 2.048. Accuracy is 59.30%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 1.975. Accuracy is 63.85%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 3 seconds. Training speed 20891 pps. Validation speed 53439 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 1.907. Accuracy is 63.63%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 1.817. Accuracy is 67.54%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 3 seconds. Training speed 21015 pps. Validation speed 45721 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 1.744. Accuracy is 66.74%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 1.644. Accuracy is 70.53%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 3 seconds. Training speed 21162 pps. Validation speed 46114 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 1.574. Accuracy is 69.61%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 1.472. Accuracy is 73.28%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 3 seconds. Training speed 18630 pps. Validation speed 46048 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 1.413. Accuracy is 72.51%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 1.315. Accuracy is 75.21%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 3 seconds. Training speed 20447 pps. Validation speed 50656 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 1.272. Accuracy is 74.99%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 1.180. Accuracy is 78.58%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 3 seconds. Training speed 20439 pps. Validation speed 47340 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 1.151. Accuracy is 77.19%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 1.067. Accuracy is 80.45%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 3 seconds. Training speed 20503 pps. Validation speed 49603 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 1.051. Accuracy is 78.97%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.973. Accuracy is 82.15%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 3 seconds. Training speed 20644 pps. Validation speed 46164 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.968. Accuracy is 80.36%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.895. Accuracy is 83.42%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 3 seconds. Training speed 19370 pps. Validation speed 47908 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.899. Accuracy is 81.51%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.831. Accuracy is 84.33%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 3 seconds. Training speed 17197 pps. Validation speed 42790 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.841. Accuracy is 82.38%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.777. Accuracy is 84.93%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 3 seconds. Training speed 18889 pps. Validation speed 40117 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.792. Accuracy is 83.15%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.731. Accuracy is 85.60%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 3 seconds. Training speed 19839 pps. Validation speed 51808 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.750. Accuracy is 83.81%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.691. Accuracy is 86.07%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 3 seconds. Training speed 20707 pps. Validation speed 49807 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.714. Accuracy is 84.32%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.657. Accuracy is 86.58%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 3 seconds. Training speed 20644 pps. Validation speed 47493 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.683. Accuracy is 84.83%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.628. Accuracy is 86.95%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 3 seconds. Training speed 20325 pps. Validation speed 48861 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.655. Accuracy is 85.18%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.602. Accuracy is 87.29%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 3 seconds. Training speed 21215 pps. Validation speed 51747 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.631. Accuracy is 85.61%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.579. Accuracy is 87.64%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 3 seconds. Training speed 20851 pps. Validation speed 50035 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.610. Accuracy is 85.92%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.559. Accuracy is 87.88%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 3 seconds. Training speed 20753 pps. Validation speed 48841 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.590. Accuracy is 86.20%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.541. Accuracy is 88.21%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 3 seconds. Training speed 20279 pps. Validation speed 50131 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.573. Accuracy is 86.46%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.525. Accuracy is 88.32%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 3 seconds. Training speed 19500 pps. Validation speed 46566 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.557. Accuracy is 86.71%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.510. Accuracy is 88.44%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 3 seconds. Training speed 20700 pps. Validation speed 51457 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.543. Accuracy is 86.92%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.497. Accuracy is 88.61%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 3 seconds. Training speed 19731 pps. Validation speed 50774 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.530. Accuracy is 87.10%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.485. Accuracy is 88.77%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 3 seconds. Training speed 19672 pps. Validation speed 46831 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.519. Accuracy is 87.26%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.474. Accuracy is 88.92%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 3 seconds. Training speed 19428 pps. Validation speed 51832 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.508. Accuracy is 87.46%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.464. Accuracy is 88.90%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 3 seconds. Training speed 20092 pps. Validation speed 46340 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.498. Accuracy is 87.61%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.454. Accuracy is 88.98%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 3 seconds. Training speed 19945 pps. Validation speed 53260 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.488. Accuracy is 87.73%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.446. Accuracy is 89.28%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 3 seconds. Training speed 21462 pps. Validation speed 49631 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.480. Accuracy is 87.87%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.438. Accuracy is 89.32%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 3 seconds. Training speed 20317 pps. Validation speed 51520 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 88.66 % (cost is 0.453)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ========\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for config in grid:\n",
    "    optimiser = SGDOptimiser(lr_scheduler=config['lr_scheduler'])\n",
    "    logger.info('Training started with learning rate %0.3f' % config['learning_rate'])\n",
    "    config['tr_stats'], config['valid_stats'] = \\\n",
    "        optimiser.train(config['model'], config['train_dp'], config['valid_dp'])\n",
    "    logger.info('Testing the model on test set:')\n",
    "    config['test_cost'], config['test_accuracy'] = \\\n",
    "        optimiser.validate(config['model'], config['test_dp'])\n",
    "    logger.info('MNIST test set accuracy is %.2f %% (cost is %.3f)' % \\\n",
    "                (config['test_accuracy']*100., config['test_cost']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>test_cost</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.076950</td>\n",
       "      <td>0.9768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.100818</td>\n",
       "      <td>0.9704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.144329</td>\n",
       "      <td>0.9577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.203264</td>\n",
       "      <td>0.9407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.340387</td>\n",
       "      <td>0.9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.453201</td>\n",
       "      <td>0.8866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  test_cost  test_accuracy\n",
       "0          0.500   0.076950         0.9768\n",
       "1          0.200   0.100818         0.9704\n",
       "2          0.100   0.144329         0.9577\n",
       "3          0.050   0.203264         0.9407\n",
       "4          0.010   0.340387         0.9057\n",
       "5          0.005   0.453201         0.8866"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEKCAYAAADticXcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n55wrIx7UnXOujHhQd865MvL/NiMf7FoFtu0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11eacad50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEKCAYAAADticXcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIp7H4+K1jwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aa7d050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Plot (on a single graph) the error rate curves for each learning rate as a function of training epochs for training set\n",
    "tmp = [m['tr_stats'] for m in grid]\n",
    "df = pd.DataFrame(np.array([np.array(i)[:,1] for i in tmp]).T, columns=learning_rates)\n",
    "df.plot().set_title('Training Accuracy (%)')\n",
    "plt.savefig(\"/Users/kungfujam/git/mlpractical/data/figures/training_accuracy.eps\", dpi=300)\n",
    "\n",
    "# Plot (on another single graph) the error rate curves as a function of training epochs for validation set\n",
    "tmp = [m['valid_stats'] for m in grid]\n",
    "df = pd.DataFrame(np.array([np.array(i)[:,1] for i in tmp]).T, columns=learning_rates)\n",
    "df.plot().set_title('Validation Accuracy (%)')\n",
    "plt.savefig(\"/Users/kungfujam/git/mlpractical/data/figures/validation_accuracy.eps\", dpi=300)\n",
    "\n",
    "# Include a table of the corresponding error rates for test set\n",
    "testres = pd.DataFrame(grid)[['learning_rate', 'test_cost', 'test_accuracy']]\n",
    "testres\n",
    "\n",
    "# plt.subplot(211)\n",
    "# for learning_rate\n",
    "# plt.plot([1,2,3], label=\"test1\")\n",
    "# plt.plot([3,2,1], label=\"test2\")\n",
    "# # Place a legend above this legend, expanding itself to\n",
    "# # fully use the given bounding box.\n",
    "# plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "#            ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "# plt.subplot(223)\n",
    "# plt.plot([1,2,3], label=\"test1\")\n",
    "# plt.plot([3,2,1], label=\"test2\")\n",
    "# # Place a legend to the right of this smaller figure.\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The training and validation accuracy increase as the learning increased. The test set accuracy is best for the fastest learning rate. This suggests we may need to run the models for longer to see benefits of lower rates._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** (10%) Plot the following graphs:\n",
    "  * Display the 784-element weight vector of each of the 100 hidden units as 10x10 grid plot of 28x28 images, in order to visualise what features of the input they are encoding.  To do this, take the weight vector of each hidden unit, reshape to 28x28, and plot using the `imshow` function).\n",
    "  * Plot a Hinton Diagram of the output layer weight matrix for digits 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Function `hinton` adapted from the matplotlib examples_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    if not max_weight:\n",
    "        max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    for (x,y),w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w))\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigmoid_Ws = [config['model'].layers[0].W for config in grid]\n",
    "\n",
    "# So that all plots are relative, find the min and max values over all models\n",
    "max_w = np.max([(np.max(W), np.min(W)) for W in sigmoid_Ws])\n",
    "min_w = np.min([(np.max(W), np.min(W)) for W in sigmoid_Ws])\n",
    "\n",
    "plt.close('all')\n",
    "for i, W in enumerate(sigmoid_Ws):\n",
    "    fig, axList = plt.subplots(10, 10)  # There are 100 weights in the layer\n",
    "    fig.set_size_inches(14, 14)\n",
    "    axList = axList.flatten()\n",
    "    for j, ax in enumerate(axList):\n",
    "        hinton(W[:,j].reshape(28,28), max_weight=max_w, ax=ax)\n",
    "    fig.suptitle(\"Model weights for learning rate %0.3f\" % learning_rates[i])\n",
    "    fig.savefig(\"/Users/kungfujam/git/mlpractical/data/figures/sig_weights_hinton__lr%0.3f.eps\" \\\n",
    "                %learning_rates[i], dpi=300)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** (cont.) (10%) Plot the following graphs:\n",
    "    * Plot a Hinton Diagram of the output layer weight matrix for digits 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softmax_Ws = [config['model'].layers[1].W for config in grid]\n",
    "\n",
    "plt.close('all')\n",
    "for i, W in enumerate(softmax_Ws):\n",
    "    fig, axList = plt.subplots(1, 2)  # we only want to show two weight matrices:\n",
    "    fig.set_size_inches(14, 14)       #   the first couple - zero and one\n",
    "    axList = axList.flatten()\n",
    "    for j, ax in enumerate(axList):\n",
    "        hinton(W[:,j].reshape(10,10), max_weight=max_w, ax=ax)\n",
    "    fig.suptitle(\"Model weights for learning rate %0.3f\" % learning_rates[i])\n",
    "    fig.savefig(\"/Users/kungfujam/git/mlpractical/data/figures/soft_weights_hinton__lr%0.3f.eps\" \\\n",
    "                %learning_rates[i], dpi=300)\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "# TODO: what you really want to do is show a weighted sum of the hidden units..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 4 - Experiments with 1-5 hidden layers (30%)\n",
    "\n",
    "In this task use the learning rate which resulted in the best accuracy in your experiments in Task 3 (b).  Perform the following experiments:\n",
    "\n",
    "  * Train a similar model to Task 3, with one hidden layer, but with 800 hidden units. \n",
    "  * Train 4 additional models with 2, 3, 4 and 5 hidden layers.  Set the number of hidden units for each model, such that all the models have similar number of trainable weights ($\\pm$2%).   For simplicity, for a given model, keep the number of units in each hidden layer the same.\n",
    "  * Plot value of the error function for training and validation sets as a function of training epochs for each model\n",
    "  * Plot the test set classification accuracy as a function of the number of hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Mathematica\n",
    "In[1]:= DD = 784           (* Input *);\n",
    "K = 10                     (* Output *);\n",
    "H1 = 800                   (* First config hidden nodes *);\n",
    "P = DD H1 + H1 K + H1 + K  (* Parameters (inc. bias) *)\n",
    "\n",
    "Out[4]= 636010\n",
    "\n",
    "In[5]:= NSolve[DD*H + H^2 + H*K + DD + 2*H == P, H]\n",
    "\n",
    "Out[5]= {{H -> -1288.86}, {H -> 492.859}}\n",
    "\n",
    "In[6]:= NSolve[DD*H + 2*H^2 + H * K + DD + 3*H == P, H]\n",
    "\n",
    "Out[6]= {{H -> -797.007}, {H -> 398.507}}\n",
    "\n",
    "In[7]:= NSolve[DD*H + 3*H^2 + H * K + DD + 4*H == P, H]\n",
    "\n",
    "Out[7]= {{H -> -611.99}, {H -> 345.99}}\n",
    "\n",
    "In[8]:= NSolve[DD*H + 4*H^2 + H * K + DD + 5*H == P, H]\n",
    "\n",
    "Out[8]= {{H -> -510.705}, {H -> 310.955}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 30\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Create model objects\n",
    "# ====================\n",
    "import numpy as np\n",
    "from mlp.layers import MLP, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "from copy import deepcopy\n",
    "\n",
    "rng = np.random.RandomState([2015,10,10])\n",
    "\n",
    "# Model 1\n",
    "# =======\n",
    "nr_nodes = 800\n",
    "cost1 = CECost()\n",
    "model1 = MLP(cost=cost1)\n",
    "model1.add_layer(Sigmoid(idim=784, odim=nr_nodes, rng=rng))\n",
    "model1.add_layer(Softmax(idim=nr_nodes, odim=10, rng=rng))\n",
    "\n",
    "# Model 2\n",
    "# =======\n",
    "nr_nodes = 493\n",
    "cost2 = CECost()\n",
    "model2 = MLP(cost=cost2)\n",
    "model2.add_layer(Sigmoid(idim=784, odim=nr_nodes, rng=rng))\n",
    "model2.add_layer(Sigmoid(idim=nr_nodes, odim=nr_nodes, rng=rng))\n",
    "model2.add_layer(Softmax(idim=nr_nodes, odim=10, rng=rng))\n",
    "\n",
    "# Model 3\n",
    "# =======\n",
    "nr_nodes = 399\n",
    "cost3 = CECost()\n",
    "model3 = MLP(cost=cost3)\n",
    "model3.add_layer(Sigmoid(idim=784, odim=nr_nodes, rng=rng))\n",
    "model3.add_layer(Sigmoid(idim=nr_nodes, odim=nr_nodes, rng=rng))\n",
    "model3.add_layer(Sigmoid(idim=nr_nodes, odim=nr_nodes, rng=rng))\n",
    "model3.add_layer(Softmax(idim=nr_nodes, odim=10, rng=rng))\n",
    "\n",
    "# Model 4\n",
    "# =======\n",
    "nr_nodes = 346\n",
    "cost4 = CECost()\n",
    "model4 = MLP(cost=cost4)\n",
    "model4.add_layer(Sigmoid(idim=784, odim=nr_nodes, rng=rng))\n",
    "model4.add_layer(Sigmoid(idim=nr_nodes, odim=nr_nodes, rng=rng))\n",
    "model4.add_layer(Sigmoid(idim=nr_nodes, odim=nr_nodes, rng=rng))\n",
    "model4.add_layer(Sigmoid(idim=nr_nodes, odim=nr_nodes, rng=rng))\n",
    "model4.add_layer(Softmax(idim=nr_nodes, odim=10, rng=rng))\n",
    "\n",
    "# Model 5\n",
    "# =======\n",
    "nr_nodes = 311\n",
    "cost5 = CECost()\n",
    "model5 = MLP(cost=cost5)\n",
    "model5.add_layer(Sigmoid(idim=784, odim=nr_nodes, rng=rng))\n",
    "model5.add_layer(Sigmoid(idim=nr_nodes, odim=nr_nodes, rng=rng))\n",
    "model5.add_layer(Sigmoid(idim=nr_nodes, odim=nr_nodes, rng=rng))\n",
    "model5.add_layer(Sigmoid(idim=nr_nodes, odim=nr_nodes, rng=rng))\n",
    "model5.add_layer(Softmax(idim=nr_nodes, odim=10, rng=rng))\n",
    "\n",
    "models = [model1, model2, model3, model4, model5]\n",
    "# Data\n",
    "# ====\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=-1, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=100, max_num_batches=-1, randomize=False)\n",
    "\n",
    "constantParams = {\n",
    "    'train_dp': train_dp,\n",
    "    'valid_dp': valid_dp,\n",
    "    'test_dp': test_dp,\n",
    "    'lr_scheduler': LearningRateFixed(learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "}\n",
    "\n",
    "grid = [\n",
    "    dict(\n",
    "        deepcopy(constantParams).items() +\n",
    "        [\n",
    "            ('model', model),\n",
    "            ('learning_rate', learning_rate),\n",
    "            ('tr_stats', []),\n",
    "            ('valid_stats', []),\n",
    "            ('test_cost', None),\n",
    "            ('test_accuracy', None)\n",
    "        ]\n",
    "    )\n",
    "    for models in models\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# ========\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "for i, config in enumerate(grid):\n",
    "    optimiser = SGDOptimiser(lr_scheduler=config['lr_scheduler'])\n",
    "    logger.info('Training Model %d started with learning rate %0.3f' % \\\n",
    "                (i, config['learning_rate']))\n",
    "    config['tr_stats'], config['valid_stats'] = \\\n",
    "        optimiser.train(config['model'], config['train_dp'], config['valid_dp'])\n",
    "    logger.info('Testing the model on test set:')\n",
    "    config['test_cost'], config['test_accuracy'] = \\\n",
    "        optimiser.validate(config['model'], config['test_dp'])\n",
    "    logger.info('MNIST test set accuracy is %.2f %% (cost is %.3f)' % \\\n",
    "                (config['test_accuracy']*100., config['test_cost']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the end of coursework 1.\n",
    "\n",
    "Please remember to save your notebook, and submit your notebook following the instructions at the top.  Please make sure that you have executed all the code cells when you submit the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
